- Duplicate Matmul Functions - KEEP AS-IS for performance reasons

---

Issue: atomicAdd(&dQ[offset], dQi[i]) is not valid WGSL

Location: FLASHATTENTION_BACKWARD_KERNEL

Problem: WGSL doesn't support atomicAdd on f32 types

Fix: Either:

    Use atomic<u32> with bitcast/reinterpret operations

    Redesign the algorithm to avoid atomic operations on floats

    Use workgroup-level accumulation instead


Logical Bugs to Investigate
=======================
Layer Norm Backward (gpu_ops_backward.py)
- Zeroes out gradgamma and gradbeta buffers BEFORE kernel
- Kernel comments say "Uses atomic-free accumulation"
- But WebGPU comment says "doesn't have atomics, so use fallback"
- Potential race condition if multiple elements write to same gamma/beta grads

Attention Kernel Sizing
- MULTIHEADATTENTIONKERNEL has hardcoded array sizes: var scores: array<f32, 512>
- Will fail if seqlen > 512
- FLASHATTENTIONFORWARDKERNEL has similar issue with const Bc = 32u; const Br = 32u

Buffer Pool Release Logic (gpu_buffer.py)
- pool_release_buffer checks if bufferid in poolstate.inuse but doesn't handle case where it's NOT in use
- Could silently fail to return buffers
