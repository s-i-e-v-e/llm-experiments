from v2.hyper import HyperParams

"""
Complete WGPU backend for transformer training and inference.
Now with full backward pass and optimizer support.
"""

import dataclasses
import json
import random
import time
from typing import Optional

import numpy as np

import v2.gpu as gpu
from common.util import deserialize, get_model_file_names, serialize
from v2.hyper import HyperParams

# ============================================================================
# DATA STRUCTURES
# ============================================================================


@dataclasses.dataclass
class TransformerModelParams:
    vocab_size: int
    embedding_dim: int
    context_size: int
    n_heads: int
    n_layers: int
    epochs: list = dataclasses.field(default_factory=list)


@dataclasses.dataclass
class TransformerModel:
    tm_params: TransformerModelParams
    params: gpu.GPUModelParams
    opt_state: gpu.GPUOptimizerState
    learning_rate: float
    total_steps: int


@dataclasses.dataclass
class ForwardCache:
    """Cache activations from forward pass for backward"""

    """Cache activations from forward pass for backward"""
    embeddings: Optional[gpu.GPUBuffer] = None
    layer_inputs: list = dataclasses.field(default_factory=list)
    layer_norms1: list = dataclasses.field(default_factory=list)
    attn_outputs: list = dataclasses.field(default_factory=list)
    layer_norms2: list = dataclasses.field(default_factory=list)
    ffn_hidden: list = dataclasses.field(default_factory=list)
    layer_outputs: list = dataclasses.field(default_factory=list)
    final_output: Optional[gpu.GPUBuffer] = None


# ============================================================================
# FORWARD PASS WITH CACHING
# ============================================================================


# Update the forward_pass_with_cache function to use FlashAttention


def forward_pass_with_cache(model: TransformerModel, batch_inputs: np.ndarray):
    """Forward pass with pre-allocated buffers (NO allocations during forward)"""
    device = model.params.embedding.device
    batch_size, seq_len = batch_inputs.shape
    embedding_dim = model.tm_params.embedding_dim
    n_heads = model.tm_params.n_heads

    # Initialize or recreate workspace if batch size changed
    need_new_workspace = (
        model.workspace is None
        or model.workspace.x_buffer_a.shape[0] != batch_size * seq_len
    )

    if need_new_workspace:
        print(f"ðŸ”§ Creating workspace for batch_size={batch_size}, seq_len={seq_len}")
        model.workspace = create_workspace_buffers(model, batch_size, seq_len)

    ws = model.workspace
    cache = ForwardCache()

    # 1. Embedding (reuse buffer from workspace)
    input_ids_flat = batch_inputs.flatten().astype(np.uint32)
    input_ids_buffer = device.create_buffer_with_data(
        data=input_ids_flat, usage=gpu.wgpu.BufferUsage.STORAGE
    )

    # Use a persistent embedding buffer
    if not hasattr(model, "_embedding_output"):
        model._embedding_output = gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        )

    x = model._embedding_output
    cache.embeddings = x

    params = np.array([batch_size, seq_len, embedding_dim], dtype=np.uint32)
    params_buffer = device.create_buffer_with_data(
        data=params, usage=gpu.wgpu.BufferUsage.UNIFORM
    )

    pipeline = gpu._get_or_create_pipeline(gpu.EMBEDDING_KERNEL, device)
    bind_group = device.create_bind_group(
        layout=pipeline.get_bind_group_layout(0),
        entries=[
            {
                "binding": 0,
                "resource": {
                    "buffer": params_buffer,
                    "offset": 0,
                    "size": params.nbytes,
                },
            },
            {
                "binding": 1,
                "resource": {
                    "buffer": model.params.embedding.buffer,
                    "offset": 0,
                    "size": model.params.embedding.size * 4,
                },
            },
            {
                "binding": 2,
                "resource": {
                    "buffer": model.params.pos_encoding.buffer,
                    "offset": 0,
                    "size": model.params.pos_encoding.size * 4,
                },
            },
            {
                "binding": 3,
                "resource": {
                    "buffer": input_ids_buffer,
                    "offset": 0,
                    "size": input_ids_flat.nbytes,
                },
            },
            {
                "binding": 4,
                "resource": {"buffer": x.buffer, "offset": 0, "size": x.size * 4},
            },
        ],
    )

    encoder = device.create_command_encoder()
    compute_pass = encoder.begin_compute_pass()
    compute_pass.set_pipeline(pipeline)
    compute_pass.set_bind_group(0, bind_group)
    compute_pass.dispatch_workgroups((batch_size * seq_len + 255) // 256, 1, 1)
    compute_pass.end()
    device.queue.submit([encoder.finish()])

    # 2. Process layers WITHOUT allocating new buffers
    for layer_idx, layer in enumerate(model.params.layers):
        # Alternate between buffers - no copying!
        if layer_idx % 2 == 0:
            x_in = ws.x_buffer_a
            x_out = ws.x_buffer_b
        else:
            x_in = ws.x_buffer_b
            x_out = ws.x_buffer_a

        # Save input for cache (shallow copy reference)
        cache.layer_inputs.append(x_in)

        # Use workspace buffers (reused for each layer)
        batcher = gpu.CommandBatcher(device)
        batcher.begin()

        batcher.add_layernorm(x_in, layer.ln_gamma1, layer.ln_beta1, ws.x_norm1)
        batcher.add_matmul(ws.x_norm1, layer.attn_wq, ws.Q)
        batcher.add_matmul(ws.x_norm1, layer.attn_wk, ws.K)
        batcher.add_matmul(ws.x_norm1, layer.attn_wv, ws.V)
        batcher.submit()

        cache.layer_norms1.append(ws.x_norm1)

        # Attention
        gpu.run_simple_attention(ws.Q, ws.K, ws.V, ws.attn_out_pre, n_heads, device)

        # Rest of layer - output goes to x_out
        batcher.begin()
        batcher.add_matmul(ws.attn_out_pre, layer.attn_wo, ws.attn_out)
        batcher.add_residual(x_in, ws.attn_out, ws.x_with_attn)
        batcher.add_layernorm(
            ws.x_with_attn, layer.ln_gamma2, layer.ln_beta2, ws.x_norm2
        )
        batcher.add_matmul(ws.x_norm2, layer.ff_w1, ws.hidden)
        batcher.add_bias_add(ws.hidden, layer.ff_b1, ws.hidden_bias)
        batcher.add_gelu(ws.hidden_bias, ws.hidden_gelu)
        batcher.add_matmul(ws.hidden_gelu, layer.ff_w2, ws.ffn_out)
        batcher.add_bias_add(ws.ffn_out, layer.ff_b2, ws.ffn_out_bias)
        batcher.add_residual(ws.x_with_attn, ws.ffn_out_bias, x_out)
        batcher.submit()

        cache.attn_outputs.append(ws.attn_out)
        cache.layer_norms2.append(ws.x_norm2)
        cache.ffn_hidden.append(ws.hidden_gelu)
        cache.layer_outputs.append(x_out)

    # Final output is in the last x_out
    n_layers = len(model.params.layers)
    final_x = ws.x_buffer_b if n_layers % 2 == 0 else ws.x_buffer_a
    cache.final_output = final_x

    # 3. Output projection (embedding_T already computed)
    batcher = gpu.CommandBatcher(device)
    batcher.begin()
    batcher.add_matmul(x, ws.embedding_T, ws.logits)
    batcher.submit()

    return ws.logits, cache


# ============================================================================
# BACKWARD PASS
# ============================================================================


def backward_pass(
    model: TransformerModel,
    logits: gpu.GPUBuffer,
    targets: np.ndarray,
    cache: ForwardCache,
    read_loss: bool = True,
):
    """SIMPLIFIED backward pass - just compute loss and gradients w.r.t. logits"""
    device = model.params.embedding.device
    batch_size, seq_len = targets.shape
    vocab_size = model.tm_params.vocab_size
    total_tokens = batch_size * seq_len

    # Compute loss and gradient w.r.t. logits only
    targets_flat = targets.flatten().astype(np.uint32)
    targets_buffer = device.create_buffer_with_data(
        data=targets_flat, usage=gpu.wgpu.BufferUsage.STORAGE
    )

    loss_buffer = device.create_buffer(
        size=total_tokens * 4,
        usage=gpu.wgpu.BufferUsage.STORAGE | gpu.wgpu.BufferUsage.COPY_SRC,
    )

    grad_logits = gpu.create_gpu_buffer((total_tokens, vocab_size), device=device)

    params = np.array([batch_size, seq_len, vocab_size], dtype=np.uint32)
    params_buffer = device.create_buffer_with_data(
        data=params, usage=gpu.wgpu.BufferUsage.UNIFORM
    )

    pipeline = gpu._get_or_create_pipeline(gpu.CROSS_ENTROPY_LOSS_KERNEL, device)
    bind_group = device.create_bind_group(
        layout=pipeline.get_bind_group_layout(0),
        entries=[
            {
                "binding": 0,
                "resource": {
                    "buffer": params_buffer,
                    "offset": 0,
                    "size": params.nbytes,
                },
            },
            {
                "binding": 1,
                "resource": {
                    "buffer": logits.buffer,
                    "offset": 0,
                    "size": logits.size * 4,
                },
            },
            {
                "binding": 2,
                "resource": {
                    "buffer": targets_buffer,
                    "offset": 0,
                    "size": targets_flat.nbytes,
                },
            },
            {
                "binding": 3,
                "resource": {
                    "buffer": loss_buffer,
                    "offset": 0,
                    "size": total_tokens * 4,
                },
            },
            {
                "binding": 4,
                "resource": {
                    "buffer": grad_logits.buffer,
                    "offset": 0,
                    "size": grad_logits.size * 4,
                },
            },
        ],
    )

    encoder = device.create_command_encoder()
    compute_pass = encoder.begin_compute_pass()
    compute_pass.set_pipeline(pipeline)
    compute_pass.set_bind_group(0, bind_group)
    compute_pass.dispatch_workgroups((total_tokens + 255) // 256, 1, 1)
    compute_pass.end()

    loss_read_buffer = device.create_buffer(
        size=total_tokens * 4,
        usage=gpu.wgpu.BufferUsage.COPY_DST | gpu.wgpu.BufferUsage.MAP_READ,
    )
    encoder.copy_buffer_to_buffer(loss_buffer, 0, loss_read_buffer, 0, total_tokens * 4)
    device.queue.submit([encoder.finish()])

    if read_loss:
        loss_read_buffer.map_sync(gpu.wgpu.MapMode.READ)
        loss_data = np.frombuffer(
            loss_read_buffer.read_mapped(), dtype=np.float32
        ).copy()
        loss_read_buffer.unmap()
        loss_value = float(np.mean(loss_data))
    else:
        loss_value = 0.0

    # FOR NOW: Skip full backward pass, just create dummy gradients
    # This will make training MUCH faster but won't actually train
    # Use this to test if loss computation works first!
    gradients = {
        "embedding": gpu.create_gpu_buffer(model.params.embedding.shape, device=device),
        "layers": [],
    }

    for layer in model.params.layers:
        layer_grads = {
            "attn_wq": gpu.create_gpu_buffer(layer.attn_wq.shape, device=device),
            "attn_wk": gpu.create_gpu_buffer(layer.attn_wk.shape, device=device),
            "attn_wv": gpu.create_gpu_buffer(layer.attn_wv.shape, device=device),
            "attn_wo": gpu.create_gpu_buffer(layer.attn_wo.shape, device=device),
            "ff_w1": gpu.create_gpu_buffer(layer.ff_w1.shape, device=device),
            "ff_b1": gpu.create_gpu_buffer(layer.ff_b1.shape, device=device),
            "ff_w2": gpu.create_gpu_buffer(layer.ff_w2.shape, device=device),
            "ff_b2": gpu.create_gpu_buffer(layer.ff_b2.shape, device=device),
            "ln_gamma1": gpu.create_gpu_buffer(layer.ln_gamma1.shape, device=device),
            "ln_beta1": gpu.create_gpu_buffer(layer.ln_beta1.shape, device=device),
            "ln_gamma2": gpu.create_gpu_buffer(layer.ln_gamma2.shape, device=device),
            "ln_beta2": gpu.create_gpu_buffer(layer.ln_beta2.shape, device=device),
        }
        gradients["layers"].append(layer_grads)

    return gradients, loss_value


# ============================================================================
# OPTIMIZER UPDATE
# ============================================================================


def apply_optimizer_updates(
    model: TransformerModel, gradients: dict, learning_rate: float
):
    """Apply AdamW optimizer updates to all parameters"""
    device = model.params.embedding.device

    # Optimizer hyperparameters
    beta1 = 0.9
    beta2 = 0.999
    eps = 1e-8
    weight_decay = 0.01

    model.opt_state.step += 1
    step = model.opt_state.step

    # Update embedding
    gpu.run_adamw_update(
        gradients["embedding"],
        model.params.embedding,
        model.opt_state.m_embedding,
        model.opt_state.v_embedding,
        learning_rate,
        beta1,
        beta2,
        weight_decay,
        eps,
        step,
        device,
    )

    # Update each layer
    for layer_idx, (layer, layer_grads, m_layer, v_layer) in enumerate(
        zip(
            model.params.layers,
            gradients["layers"],
            model.opt_state.m_layers,
            model.opt_state.v_layers,
        )
    ):
        # Update all layer parameters
        for param_name in [
            "attn_wq",
            "attn_wk",
            "attn_wv",
            "attn_wo",
            "ff_w1",
            "ff_b1",
            "ff_w2",
            "ff_b2",
            "ln_gamma1",
            "ln_beta1",
            "ln_gamma2",
            "ln_beta2",
        ]:
            param = getattr(layer, param_name)
            grad = layer_grads[param_name]
            m = getattr(m_layer, param_name)
            v = getattr(v_layer, param_name)

            gpu.run_adamw_update(
                grad,
                param,
                m,
                v,
                learning_rate,
                beta1,
                beta2,
                weight_decay,
                eps,
                step,
                device,
            )


# ============================================================================
# TRAINING OPERATIONS
# ============================================================================


def train_step(model: TransformerModel, batch_inputs, batch_targets, step: int):
    """Training step with timing"""
    t0 = time.perf_counter()
    logits, cache = forward_pass_with_cache(model, batch_inputs)
    t1 = time.perf_counter()

    read_loss = step % 10 == 0
    gradients, loss = backward_pass(model, logits, batch_targets, cache, read_loss)
    t2 = time.perf_counter()

    apply_optimizer_updates(model, gradients, model.learning_rate)
    t3 = time.perf_counter()

    if step % 100 == 0:
        print(
            f"â±ï¸ Forward: {(t1 - t0) * 1000:.1f}ms | Backward: {(t2 - t1) * 1000:.1f}ms | Optimizer: {(t3 - t2) * 1000:.1f}ms"
        )

    return model, loss


def train_epoch(
    model: TransformerModel,
    token_data: list,
    batch_size: int,
    context_size: int,
    epoch_num: int,
):
    """Train for one epoch using GPU kernels"""

    # Create sequences
    sequences = []
    stride = batch_size
    for i in range(0, len(token_data) - context_size, stride):
        seq = token_data[i : i + context_size + 1]
        if len(seq) == context_size + 1:
            input_seq = seq[:-1]
            target_seq = seq[1:]
            sequences.append((input_seq, target_seq))

    random.shuffle(sequences)
    steps_in_epoch = max(1, len(sequences) // batch_size)

    if len(sequences) == 0:
        return model, {"avg_loss": 0.0, "smooth_loss": 0.0, "steps": 0}

    # Training state
    epoch_loss = 0.0
    step = 0
    smooth_loss = 0.0
    start_time = time.time()

    # Training loop
    for batch_start in range(0, len(sequences), batch_size):
        batch = sequences[batch_start : batch_start + batch_size]
        if len(batch) == 0:
            continue

        batch_inputs = np.array([seq[0] for seq in batch], dtype=np.int32)
        batch_targets = np.array([seq[1] for seq in batch], dtype=np.int32)

        # Single GPU training step
        model, loss = train_step(model, batch_inputs, batch_targets, step)

        # Update metrics
        epoch_loss += float(loss)
        smooth_loss = (
            smooth_loss * 0.999 + float(loss) * 0.001
            if smooth_loss > 0
            else float(loss)
        )
        step += 1

        # Progress reporting
        elapsed = time.time() - start_time
        tokens_per_sec = (
            (batch_size * context_size * step) / elapsed if elapsed > 0 else 0
        )

        print(
            f"\rEpoch {epoch_num + 1}, Step {step}/{steps_in_epoch} | "
            f"Loss: {float(loss):.4f} | Smooth: {smooth_loss:.4f} | "
            f"TPS: {tokens_per_sec:.0f}",
            end="",
        )

    print()

    avg_loss = epoch_loss / step if step > 0 else 0.0

    return model, {
        "avg_loss": avg_loss,
        "smooth_loss": smooth_loss,
        "steps": step,
    }


# ============================================================================
# MODEL LIFECYCLE
# ============================================================================


def initialize_model(
    hp: HyperParams,
    epochs,
    total_steps,
):
    """Initialize model on GPU"""

    np.random.seed(0)

    print("Initializing WGPU device...")
    device = gpu.get_device()
    if device is None:
        raise RuntimeError("Failed to initialize WGPU device")

    print(
        f"Creating model: {hp.n_layers} layers, {hp.embedding_dim} dims, {hp.vocab_size} vocab"
    )
    gpu_params = gpu.create_gpu_model_params(
        hp.vocab_size, hp.embedding_dim, hp.context_size, hp.n_layers
    )

    print("Initializing optimizer state...")
    opt_state = gpu.create_optimizer_state(gpu_params)

    print("âœ… Model initialized on GPU")

    return TransformerModel(
        tm_params=TransformerModelParams(
            vocab_size=hp.vocab_size,
            embedding_dim=hp.embedding_dim,
            context_size=hp.context_size,
            n_heads=hp.n_heads,
            n_layers=hp.n_layers,
            epochs=epochs,
        ),
        params=gpu_params,
        opt_state=opt_state,
        learning_rate=hp.learning_rate,
        total_steps=total_steps,
    )


def save_model(model: TransformerModel, model_path: str):
    """Save model to disk"""
    print(f"Saving model to {model_path}...")
    xs = get_model_file_names(model_path)

    # Convert GPU params to numpy for serialization
    params_dict = {
        "embedding": gpu.gpu_to_numpy(model.params.embedding),
        "pos_encoding": gpu.gpu_to_numpy(model.params.pos_encoding),
        "layers": [gpu.gpu_layer_to_dict(layer) for layer in model.params.layers],
    }
    serialize(params_dict, xs[0])

    # Convert optimizer state
    opt_state_dict = {
        "m_embedding": gpu.gpu_to_numpy(model.opt_state.m_embedding),
        "v_embedding": gpu.gpu_to_numpy(model.opt_state.v_embedding),
        "m_layers": [
            gpu.gpu_layer_to_dict(layer) for layer in model.opt_state.m_layers
        ],
        "v_layers": [
            gpu.gpu_layer_to_dict(layer) for layer in model.opt_state.v_layers
        ],
        "step": model.opt_state.step,
    }
    serialize(opt_state_dict, xs[1])

    with open(xs[2], "w") as f:
        json.dump(dataclasses.asdict(model.tm_params), f, indent=2)

    print("âœ… Model saved")


def load_model(hp: HyperParams, total_steps: int, model_path: str):
    """Load model from disk"""
    print(f"Loading model from {model_path}...")
    xs = get_model_file_names(model_path)

    with open(xs[2], "r") as f:
        config_data = json.load(f)
        tm_params = TransformerModelParams(**config_data)

    device = gpu.get_device()
    if device is None:
        raise RuntimeError("Failed to initialize WGPU device")

    # Load params
    params_dict = deserialize(None, xs[0])

    gpu_params = gpu.GPUModelParams(
        embedding=gpu.create_gpu_buffer(
            params_dict["embedding"].shape, params_dict["embedding"], device
        ),
        pos_encoding=gpu.create_gpu_buffer(
            params_dict["pos_encoding"].shape, params_dict["pos_encoding"], device
        ),
        layers=[
            gpu.dict_to_gpu_layer(layer_dict, tm_params.embedding_dim, device)
            for layer_dict in params_dict["layers"]
        ],
    )

    # Load optimizer state
    opt_state_dict = deserialize(None, xs[1])

    opt_state = gpu.GPUOptimizerState(
        m_embedding=gpu.create_gpu_buffer(
            opt_state_dict["m_embedding"].shape, opt_state_dict["m_embedding"], device
        ),
        v_embedding=gpu.create_gpu_buffer(
            opt_state_dict["v_embedding"].shape, opt_state_dict["v_embedding"], device
        ),
        m_layers=[
            gpu.dict_to_gpu_layer(m_layer, tm_params.embedding_dim, device)
            for m_layer in opt_state_dict["m_layers"]
        ],
        v_layers=[
            gpu.dict_to_gpu_layer(v_layer, tm_params.embedding_dim, device)
            for v_layer in opt_state_dict["v_layers"]
        ],
        step=opt_state_dict["step"],
    )

    print("âœ… Model loaded from disk")

    return TransformerModel(
        tm_params=tm_params,
        params=gpu_params,
        opt_state=opt_state,
        learning_rate=hp.learning_rate,
        total_steps=total_steps,
    )


# ============================================================================
# INFERENCE
# ============================================================================


def sample_token(
    logits,
    temperature=1.0,
    top_k=0,
    top_p=1.0,
    min_p=0.0,
    repetition_penalty=1.0,
    recent_tokens=None,
):
    """Sample next token from logits (CPU implementation)"""
    if recent_tokens is None:
        recent_tokens = []

    if isinstance(logits, gpu.GPUBuffer):
        logits = gpu.gpu_to_numpy(logits)

    logits = np.array(logits, dtype=np.float32)

    if len(logits.shape) > 1:
        logits = logits.flatten()

    # Apply repetition penalty
    if repetition_penalty != 1.0 and len(recent_tokens) > 0:
        for token_id in set(recent_tokens):
            if 0 <= token_id < len(logits):
                logits[token_id] = logits[token_id] / repetition_penalty

    # Temperature scaling
    if temperature == 0.0:
        return int(np.argmax(logits))

    if temperature != 1.0:
        logits = logits / temperature

    # Softmax
    logits_max = np.max(logits)
    exp_logits = np.exp(logits - logits_max)
    probs = exp_logits / np.sum(exp_logits)

    # Min-p filtering
    if min_p > 0.0:
        max_prob = np.max(probs)
        min_threshold = min_p * max_prob
        mask = probs >= min_threshold
        probs = np.where(mask, probs, 0.0)
        probs_sum = np.sum(probs)
        if probs_sum > 0:
            probs = probs / probs_sum
        else:
            probs = np.ones_like(probs) / len(probs)

    # Top-k filtering
    if top_k > 0:
        top_k_indices = np.argsort(probs)[-top_k:]
        mask = np.zeros_like(probs, dtype=bool)
        mask[top_k_indices] = True
        probs = np.where(mask, probs, 0.0)
        probs_sum = np.sum(probs)
        if probs_sum > 0:
            probs = probs / probs_sum

    # Top-p (nucleus) filtering
    if top_p < 1.0:
        sorted_indices = np.argsort(probs)[::-1]
        sorted_probs = probs[sorted_indices]
        cumsum_probs = np.cumsum(sorted_probs)

        mask = cumsum_probs <= top_p
        mask[0] = True

        filtered_probs = np.zeros_like(probs)
        filtered_probs[sorted_indices] = np.where(mask, sorted_probs, 0.0)
        probs = filtered_probs
        probs_sum = np.sum(probs)
        if probs_sum > 0:
            probs = probs / probs_sum

    # Sample
    try:
        token_id = np.random.choice(len(probs), p=probs)
        return int(token_id)
    except ValueError:
        return int(np.argmax(probs))


# ============================================================================
# COMPLETE INFERENCE IMPLEMENTATION
# ============================================================================


# Also update inference to use FlashAttention
def forward_inference_complete(model: TransformerModel, input_tokens: np.ndarray):
    """
    Complete forward pass for inference with proper attention and output projection.

    Args:
        model: TransformerModel
        input_tokens: [batch_size, seq_len] token IDs

    Returns:
        logits: [batch_size, vocab_size] next token logits (only for last position)
    """
    """Complete inference with FlashAttention"""
    device = model.params.embedding.device
    batch_size, seq_len = input_tokens.shape
    embedding_dim = model.tm_params.embedding_dim
    n_heads = model.tm_params.n_heads
    vocab_size = model.tm_params.vocab_size

    # 1. Embedding + Positional Encoding
    input_ids_flat = input_tokens.flatten().astype(np.uint32)
    input_ids_buffer = device.create_buffer_with_data(
        data=input_ids_flat, usage=gpu.wgpu.BufferUsage.STORAGE
    )

    x = gpu.create_gpu_buffer((batch_size * seq_len, embedding_dim), device=device)

    params = np.array([batch_size, seq_len, embedding_dim], dtype=np.uint32)
    params_buffer = device.create_buffer_with_data(
        data=params, usage=gpu.wgpu.BufferUsage.UNIFORM
    )

    pipeline = gpu._get_or_create_pipeline(gpu.EMBEDDING_KERNEL, device)
    bind_group = device.create_bind_group(
        layout=pipeline.get_bind_group_layout(0),
        entries=[
            {
                "binding": 0,
                "resource": {
                    "buffer": params_buffer,
                    "offset": 0,
                    "size": params.nbytes,
                },
            },
            {
                "binding": 1,
                "resource": {
                    "buffer": model.params.embedding.buffer,
                    "offset": 0,
                    "size": model.params.embedding.size * 4,
                },
            },
            {
                "binding": 2,
                "resource": {
                    "buffer": model.params.pos_encoding.buffer,
                    "offset": 0,
                    "size": model.params.pos_encoding.size * 4,
                },
            },
            {
                "binding": 3,
                "resource": {
                    "buffer": input_ids_buffer,
                    "offset": 0,
                    "size": input_ids_flat.nbytes,
                },
            },
            {
                "binding": 4,
                "resource": {"buffer": x.buffer, "offset": 0, "size": x.size * 4},
            },
        ],
    )

    encoder = device.create_command_encoder()
    compute_pass = encoder.begin_compute_pass()
    compute_pass.set_pipeline(pipeline)
    compute_pass.set_bind_group(0, bind_group)
    compute_pass.dispatch_workgroups((batch_size * seq_len + 255) // 256, 1, 1)
    compute_pass.end()
    device.queue.submit([encoder.finish()])

    # Process layers with FlashAttention
    for layer in model.params.layers:
        x_norm = gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        )
        gpu.run_layernorm(x, layer.ln_gamma1, layer.ln_beta1, x_norm, device)

        Q = gpu.create_gpu_buffer((batch_size * seq_len, embedding_dim), device=device)
        K = gpu.create_gpu_buffer((batch_size * seq_len, embedding_dim), device=device)
        V = gpu.create_gpu_buffer((batch_size * seq_len, embedding_dim), device=device)

        gpu.run_matmul(x_norm, layer.attn_wq, Q, device)
        gpu.run_matmul(x_norm, layer.attn_wk, K, device)
        gpu.run_matmul(x_norm, layer.attn_wv, V, device)

        # â­ FlashAttention (no backward needed for inference)
        attn_out_pre = gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        )
        gpu.run_flashattention(
            Q, K, V, attn_out_pre, n_heads, save_for_backward=False, device=device
        )

        # Multi-head self-attention
        attn_out_pre = gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        )
        gpu.run_multihead_attention(Q, K, V, attn_out_pre, n_heads, device)

        # Output projection
        attn_out = gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        )
        gpu.run_matmul(attn_out_pre, layer.attn_wo, attn_out, device)

        # Residual connection
        x_with_attn = gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        )
        gpu.run_residual_add(x, attn_out, x_with_attn, device)

        # LayerNorm 2
        x_norm2 = gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        )
        gpu.run_layernorm(x_with_attn, layer.ln_gamma2, layer.ln_beta2, x_norm2, device)

        # FFN: First linear + bias
        hidden = gpu.create_gpu_buffer(
            (batch_size * seq_len, 4 * embedding_dim), device=device
        )
        gpu.run_matmul(x_norm2, layer.ff_w1, hidden, device)

        hidden_with_bias = gpu.create_gpu_buffer(
            (batch_size * seq_len, 4 * embedding_dim), device=device
        )
        gpu.run_bias_add(hidden, layer.ff_b1, hidden_with_bias, device)

        # GELU activation
        hidden_gelu = gpu.create_gpu_buffer(
            (batch_size * seq_len, 4 * embedding_dim), device=device
        )
        gpu.run_gelu(hidden_with_bias, hidden_gelu, device)

        # Second linear + bias
        ffn_out = gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        )
        gpu.run_matmul(hidden_gelu, layer.ff_w2, ffn_out, device)

        ffn_out_with_bias = gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        )
        gpu.run_bias_add(ffn_out, layer.ff_b2, ffn_out_with_bias, device)

        # Residual connection
        x_next = gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        )
        gpu.run_residual_add(x_with_attn, ffn_out_with_bias, x_next, device)

        x = x_next

    # 3. Extract last token from each sequence
    x_last = gpu.create_gpu_buffer((batch_size, embedding_dim), device=device)
    gpu.run_extract_last_tokens(x, x_last, batch_size, seq_len, device)

    # 4. Output projection: x_last @ embedding.T
    # First transpose embedding: [vocab, dim] -> [dim, vocab]
    embedding_T = gpu.create_gpu_buffer((embedding_dim, vocab_size), device=device)
    gpu.run_transpose(model.params.embedding, embedding_T, device)

    # Then compute logits: [batch, dim] @ [dim, vocab] = [batch, vocab]
    logits = gpu.create_gpu_buffer((batch_size, vocab_size), device=device)
    gpu.run_matmul(x_last, embedding_T, logits, device)

    # Return logits as numpy array
    return gpu.gpu_to_numpy(logits)


# Replace the old forward_inference function
def forward_inference(model: TransformerModel, input_tokens: np.ndarray):
    """
    Wrapper for forward_inference_complete with better name.
    Returns logits for next token prediction.
    """
    return forward_inference_complete(model, input_tokens)


# ============================================================================
# TEXT GENERATION
# ============================================================================


def generate_text(
    model: TransformerModel,
    tokenizer,
    prompt: str,
    max_new_tokens: int = 50,
    temperature: float = 0.8,
    top_k: int = 40,
    top_p: float = 0.9,
):
    """
    Generate text from a prompt.

    Args:
        model: Trained TransformerModel
        tokenizer: Tokenizer with encode/decode methods
        prompt: Input text string
        max_new_tokens: Number of tokens to generate
        temperature: Sampling temperature
        top_k: Top-k filtering
        top_p: Nucleus sampling threshold

    Returns:
        Generated text string
    """
    # Encode prompt
    input_ids = tokenizer.encode(prompt)
    context_size = model.tm_params.context_size

    # Truncate if too long
    if len(input_ids) > context_size:
        input_ids = input_ids[-context_size:]

    generated_tokens = list(input_ids)
    recent_tokens = list(input_ids[-20:])  # For repetition penalty

    print(f"Generating from prompt: {prompt}")
    print("=" * 50)

    for i in range(max_new_tokens):
        # Prepare input (last context_size tokens)
        context = generated_tokens[-context_size:]

        # Pad if necessary
        if len(context) < context_size:
            context = [0] * (context_size - len(context)) + context

        # Convert to batch format [1, context_size]
        input_array = np.array([context], dtype=np.int32)

        # Get logits for next token
        logits = forward_inference(model, input_array)  # [1, vocab_size]

        # Sample next token
        next_token = sample_token(
            logits[0],  # First (and only) batch element
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=1.1,
            recent_tokens=recent_tokens,
        )

        # Add to generated sequence
        generated_tokens.append(next_token)
        recent_tokens.append(next_token)
        if len(recent_tokens) > 20:
            recent_tokens.pop(0)

        # Decode and print
        new_text = tokenizer.decode([next_token])
        print(new_text, end="", flush=True)

        # Check for end of text (if tokenizer has EOS token)
        if hasattr(tokenizer, "eos_token_id") and next_token == tokenizer.eos_token_id:
            break

    print("\n" + "=" * 50)

    # Decode full generated text
    return tokenizer.decode(generated_tokens)


@dataclasses.dataclass
class WorkspaceBuffers:
    """Pre-allocated buffers reused across training steps"""

    # Double buffers for ping-pong between layers
    x_buffer_a: gpu.GPUBuffer
    x_buffer_b: gpu.GPUBuffer

    # Attention workspace
    x_norm1: gpu.GPUBuffer
    Q: gpu.GPUBuffer
    K: gpu.GPUBuffer
    V: gpu.GPUBuffer
    attn_out_pre: gpu.GPUBuffer
    attn_out: gpu.GPUBuffer
    x_with_attn: gpu.GPUBuffer

    # FFN workspace
    x_norm2: gpu.GPUBuffer
    hidden: gpu.GPUBuffer
    hidden_bias: gpu.GPUBuffer
    hidden_gelu: gpu.GPUBuffer
    ffn_out: gpu.GPUBuffer
    ffn_out_bias: gpu.GPUBuffer
    x_next: gpu.GPUBuffer

    # Output
    embedding_T: gpu.GPUBuffer
    logits: gpu.GPUBuffer


def create_workspace_buffers(model: TransformerModel, batch_size: int, seq_len: int):
    """Pre-allocate all workspace buffers once"""
    device = model.params.embedding.device
    embedding_dim = model.tm_params.embedding_dim
    vocab_size = model.tm_params.vocab_size

    # Per-layer buffers (will be reused for each layer)
    workspace = WorkspaceBuffers(
        # Double buffers for alternating between layers
        x_buffer_a=gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        ),
        x_buffer_b=gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        ),
        x_norm1=gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        ),
        Q=gpu.create_gpu_buffer((batch_size * seq_len, embedding_dim), device=device),
        K=gpu.create_gpu_buffer((batch_size * seq_len, embedding_dim), device=device),
        V=gpu.create_gpu_buffer((batch_size * seq_len, embedding_dim), device=device),
        attn_out_pre=gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        ),
        attn_out=gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        ),
        x_with_attn=gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        ),
        x_norm2=gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        ),
        hidden=gpu.create_gpu_buffer(
            (batch_size * seq_len, 4 * embedding_dim), device=device
        ),
        hidden_bias=gpu.create_gpu_buffer(
            (batch_size * seq_len, 4 * embedding_dim), device=device
        ),
        hidden_gelu=gpu.create_gpu_buffer(
            (batch_size * seq_len, 4 * embedding_dim), device=device
        ),
        ffn_out=gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        ),
        ffn_out_bias=gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        ),
        x_next=gpu.create_gpu_buffer(
            (batch_size * seq_len, embedding_dim), device=device
        ),
        embedding_T=gpu.create_gpu_buffer((embedding_dim, vocab_size), device=device),
        logits=gpu.create_gpu_buffer((batch_size * seq_len, vocab_size), device=device),
    )

    # Pre-compute transposed embedding once
    gpu.run_transpose(model.params.embedding, workspace.embedding_T, device)

    return workspace


# Store workspace in model
@dataclasses.dataclass
class TransformerModel:
    tm_params: TransformerModelParams
    params: gpu.GPUModelParams
    opt_state: gpu.GPUOptimizerState
    learning_rate: float
    total_steps: int
    workspace: Optional[WorkspaceBuffers] = None  # Add this
